# 1.1. Краткая историческая справка о развитии систем передачи информации (систем связи) и теории информации (теории связи)
## 1.1.1. Краткая история развития систем связи
Развитие систем связи тесно связано с использованием электромагнитных явлений. История начинается с изобретения телеграфа и телефона.
*   **Телеграф:**
    *   **1832 г.** — Первый электромагнитный телеграф был создан русским ученым Павлом Львовичем Шиллингом (не нашел практического применения).
    *   **1833 г.** — Немецкие ученые Карл Фридрих Гаусс и Вильгельм Эдуард Вебер построили первый электромагнитный игольчатый телеграф.
    *   **1837 г.** — Американский инженер Сэмюэл Финли Бриз Морзе предложил самопишущий телеграф.
    *   **1844 г.** — Телеграф С. Морзе получил в Америке практическое применение.
    *   **1855 г.** — Английский изобретатель Дэвид Эдуард Юз создал телеграфный аппарат, который позволил передавать сообщения на большие расстояния.
*   **Телефон:**
    *   **1861 г.** — Немецкий физик и изобретатель Иоганн Филипп Рейс предложил первую конструкцию телефона.
    *   **1876 г.** — Североамериканский ученый, изобретатель и бизнесмен Александр Грэм Белл предложил конструкцию телефона, сохранившую свои основные черты до настоящего времени.
    *   **1878 г.** — Начала действовать в Хартфорде (США) первая телефонная станция.
    *   **1884 г.** — Между Нью-Йорком и Филадельфией была построена первая междугородняя телефонная линия.
    *   **1889 г.** — Американец Алмон Браун Строуджер изобрел номеронабиратель, что стало основой для создания автоматических телефонных станций (АТС).
    *   **1892 г.** — В Ла-Порте (США) была построена первая АТС.
*   **Радиосвязь:**
    *   **1867 г.** — Английский физик Джеймс Кларк Максвелл предсказал существование электромагнитных волн и предложил теорию их распространения.
    *   **1887 г.** — Немецкий физик Густав Людвиг Герц доказал существование электромагнитных волн.
    *   **1895 г.** — Русский физик и электротехник Александр Степанович Попов впервые осуществил радиоприем, применив антенну.
    *   **1902 г.** — Итальянский радиотехник и предприниматель Гульельмо Маркони принял первую радиопередачу с противоположного берега Атлантического океана.
    *   **1904 г.** — Английский физик Джон Амброуз Флеминг изобрел и запатентовал диод, что стало началом эры электроники.
*   **Телевидение:**
    *   **1924 г.** — Август Каролус использовал эффект Джоном Керра для передачи изображения.
    *   **1926 г.** — В Англии произошла первая телепередача по системе шотландского ученого Джона Лоуги Бэрда.
    *   **1950 г.** — В Америке была осуществлена первая цветная телевизионная передача.
*   **Спутниковая связь:**
    *   **1957 г.** — СССР запустил первый искусственный спутник Земли.
    *   **1962 г.** — Был запущен первый гражданский телекоммуникационный спутник «Телстар» и положено начало созданию глобальных сетей связи.
*   **Интернет и мобильная связь:**
    *   Последними достижениями конца XX и начала XXI веков в области систем связи являются создание Глобальных сетей связи (Интернет) и развитие сотовой связи.
---
## 1.1.2. О понятии «информация». Краткая история развития теории информации
### 1.1.2.1. О понятии «информация»
Определений понятия "информация" существует множество, от философского до узко практического.
*   **Философское определение:** Информация есть отражение реального мира.
*   **Практическое определение:** Информация — это все сведения, являющиеся объектом хранения, передачи и преобразования.
*   **Наиболее распространенное определение:** Информация — это характеристика не сообщения, а соотношения между сообщением и его потребителем. Без наличия потребителя говорить об информации так же бессмысленно, как и без наличия сообщения. Сообщение само по себе не содержит никакой информационной субстанции. Информация — это не материальная сущность, а способ описания взаимодействия.
### 1.1.2.2. Краткая историческая справка о развитии теории информации
Истоки теории информации можно найти в древнерусском языке, где часто слова записывали несколькими буквами. Этот принцип существует и в теории информации: чем чаще повторяют сообщение, тем меньше нужно о нем сообщать.

Основоположником современной теории информации считается американский ученый Клод Шеннон. Он заложил математические основы теории, введя количественную меру информации — энтропию.

Важный вклад в развитие теории информации внесли отечественные ученые: А. Н. Колмогоров, А. А. Харкевич, А. Я. Хинчин, Р. Л. Добрушин, Л. М. Финк, Р. Л. Стратонович, И. М. Коган, Ф. Е. Темников, В. И. Сифоров, М. С. Пинскер, А. Н. Железнов и другие, а также ряд зарубежных ученых: В. Макмиллан, А. Файнстейн, Д. Габор, Р. М. Фано, Ф. М. Вудворт, С. Голдман, Л. Бриллюэн и другие.

---
# 1.2. Информационные метрики
Это раздел, посвященный методам измерения количества информации. Оказывается, определение меры количества информации является довольно сложной задачей.
## 1.2.1. Структурные меры информации
Структурные меры основаны на анализе структуры данных и их организации.
*   **Геометрическая мера:**
    *   Представляет информацию в виде геометрической модели.
    *   Пример: Одномерную информацию можно представить как линию, двумерную — как площадь, трехмерную — как объем.
    *   Информационную емкость рассчитывают как сумму дискретных значений по всем измерениям.
*   **Комбинаторная мера:**
    *   Основана на подсчете возможных комбинаций элементов.
    *   Пример: Число размещений из `n` элементов по `m` элементов равно `A_n^m = n! / (n-m)!`.
    *   Число сочетаний с повторениями из `n` элементов по `m` элементов равно `C_{n+m-1}^m`.
*   **Аддитивная мера:**
    *   Основана на свойстве аддитивности: если два независимых источника объединяются, то неопределенность объединенного источника равна сумме неопределенностей исходных источников.
    *   Это свойство является одним из ключевых для определения энтропии как меры информации.
## 1.2.2. Статистическая мера
Это наиболее распространенная мера в теории информации. Она основана на вероятностях появления различных символов или событий.
*   **Энтропия (H):** Мера среднего количества информации, приходящегося на один символ источника. Для источника без памяти `H(X) = - Σ p_i * log₂(p_i)`.
*   **Информация (I):** Количество информации, содержащееся в конкретном событии `x_i`, определяется как `I(x_i) = -log₂(p(x_i))`. Чем меньше вероятность события, тем больше его собственная информация.
## 1.2.3. Семантическая мера
Семантическая мера учитывает смысловое содержание информации, а не только ее формальное представление. Однако в рамках данной главы она рассматривается лишь вскользь, так как основное внимание уделяется структурным и статистическим мерам.
