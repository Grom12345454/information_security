# 5.1 Префиксные коды
## Что такое префиксный код?
**Префиксный код** — это набор кодовых слов, в котором **никакое кодовое слово не является началом (префиксом) другого кодового слова**.
*   **Почему это важно?** Без этого свойства декодирование становится неоднозначным.
    *   **Пример проблемы (не префиксный код):** Пусть код: `A=0`, `B=01`, `C=1`. Получено сообщение `01`.
        *   Это может быть `A` (`0`) и `B` (`01`)? Но тогда было бы `001`.
        *   Или это просто `B` (`01`)?
        *   **Результат:** Невозможно однозначно декодировать.
    *   **Пример решения (префиксный код):** Код: `A=0`, `B=10`, `C=11`. Получено сообщение `010`.
        *   Берем первый бит: `0` → это `A`. Остаток: `10`.
        *   Берем следующие два бита: `10` → это `B`. Остаток: пусто.
        *   **Результат:** Однозначно `A B`.
## Как строить префиксные коды? Метод двоичного дерева
Это самый наглядный и фундаментальный метод.
*   **Шаг 1: Создание дерева.** Начинаем с корня.
*   **Шаг 2: Расположение символов.** Все символы источника помещаются **только в листьях** дерева (вершины без потомков).
*   **Шаг 3: Формирование кодов.** Путь от корня до листа и есть кодовое слово. Движение влево — `0`, вправо — `1`.
**Пример построения для трех символов A, B, C:**
*   **Вариант 1 (Сбалансированный):**
    ```
           Корень
          /      \
         0        1
        / \      / \
       A   B    C   (пусто)
    ```
    *   Код A: `00`
    *   Код B: `01`
    *   Код C: `10`
    *   Это префиксный код.
*   **Вариант 2 (Несбалансированный):**
    ```
           Корень
          /      \
         0        1
        / \      / \
       A   (пусто)  1
                  / \
                 B   C
    ```
    *   Код A: `00`
    *   Код B: `110`
    *   Код C: `111`
    *   Это тоже префиксный код, хотя длины разные.
**Важное правило:** Если вы поместите символ в промежуточную вершину (например, `B` в вершину `1`), то любой путь, проходящий через эту вершину, будет иметь этот символ в качестве префикса, что нарушит условие префиксности.
## Условие Фано (Алгебраическое условие)
Это формальное правило проверки префиксности.
*   **Формулировка:** Неравно-мерный код может быть однозначно декодирован, если никакой из кодов не совпадает с началом (префиксом) какого-либо другого более длинного кода.
Это просто другая формулировка определения префиксного кода. На практике это означает, что при построении кода нужно всегда проверять, не является ли новый код префиксом уже существующего, или не является ли уже существующий код префиксом нового.
## Неравенство Крафта (Теоретическая основа)
Это ключевое математическое соотношение, связывающее длины кодовых слов и возможность построения префиксного кода.
*   **Формулировка теоремы 2:** Необходимым и достаточным условием существования префиксного кода объёма M с длинами кодовых слов `l1, l2, ..., lM` является выполнение неравенства Крафта:
    `∑(i=1 to M) 2^(-li) <= 1`
*   **Что это значит?**
    *   **Необходимость:** Если код префиксный, то неравенство Крафта обязательно выполняется.
    *   **Достаточность:** Если для заданного набора длин `l1, l2, ..., lM` выполняется неравенство Крафта, то **всегда можно построить** префиксный код с этими длинами.
*   **Пример:**
    *   Рассмотрим код `{0, 10, 11}`. Длины: `l1=1, l2=2, l3=2`.
    *   Проверяем неравенство: `2^(-1) + 2^(-2) + 2^(-2) = 0.5 + 0.25 + 0.25 = 1.0 <= 1`. Условие выполняется. Код возможен (и мы его уже построили).
    *   Рассмотрим код `{0, 01, 11}`. Длины: `l1=1, l2=2, l3=2`.
    *   Проверяем неравенство: `2^(-1) + 2^(-2) + 2^(-2) = 0.5 + 0.25 + 0.25 = 1.0 <= 1`. Условие выполняется!
    *   **Важный момент:** Неравенство Крафта говорит о возможности построения *какого-то* префиксного кода с данными длинами, но **не гарантирует**, что *любой* код с этими длинами будет префиксным. В нашем случае, код `{0, 01, 11}` не префиксный, но существует другой код с теми же длинами, который префиксный, например `{0, 10, 11}`.
---
# 5.2 Построение оптимальных кодов: Алгоритмы Шеннона-Фано и Хаффмана
Оптимальный код — это код, который минимизирует среднюю длину кодового слова, учитывая вероятности появления символов. Чем чаще символ встречается, тем короче должен быть его код.
## Алгоритм Шеннона-Фано
Это один из первых алгоритмов построения оптимальных префиксных кодов.
*   **Шаг 1: Сортировка.** Символы исходного алфавита располагаются в порядке убывания их вероятностей.
*   **Шаг 2: Разбиение.** Множество символов делится на две группы так, чтобы суммарные вероятности групп были как можно ближе друг к другу.
*   **Шаг 3: Присвоение битов.** Первой группе присваивается бит `0`, второй — бит `1`.
*   **Шаг 4: Рекурсия.** Каждая группа снова делится на подгруппы (по тому же принципу), и к каждому кодовому слову добавляется новый бит (`0` для первой подгруппы, `1` для второй). Процесс повторяется до тех пор, пока в каждой подгруппе не останется по одному символу.
**Пример:**
Пусть у нас есть 6 символов с вероятностями:

| Символ | Вероятность |
|--------|-------------|
| A      | 0.3         |
| B      | 0.2         |
| C      | 0.2         |
| D      | 0.15        |
| E      | 0.1         |
| F      | 0.05        |
1.  **Сортируем:** A(0.3), B(0.2), C(0.2), D(0.15), E(0.1), F(0.05)
2.  **Первое разбиение:** Группа 1: A, B (сумма=0.5); Группа 2: C, D, E, F (сумма=0.5). Присваиваем `0` и `1`.
3.  **Разбиваем Группу 1 (A,B):** A(0.3), B(0.2). Суммы почти равны. Присваиваем `0` и `1`. Получаем коды: A=`00`, B=`01`.
4.  **Разбиваем Группу 2 (C,D,E,F):** C(0.2), D(0.15), E(0.1), F(0.05). Разбиваем на C(0.2) и D,E,F(0.3). Присваиваем `0` и `1`. Получаем: C=`10`.
5.  **Разбиваем D,E,F:** D(0.15), E(0.1), F(0.05). Разбиваем на D(0.15) и E,F(0.15). Присваиваем `0` и `1`. Получаем: D=`110`, E=`1110`, F=`1111`.
**Итоговые коды:**

| Символ | Код  |
|--------|------|
| A      | 00   |
| B      | 01   |
| C      | 10   |
| D      | 110  |
| E      | 1110 |
| F      | 1111 |
Средняя длина кода: `L = 0.3*2 + 0.2*2 + 0.2*2 + 0.15*3 + 0.1*4 + 0.05*4 = 2.45` бит/символ.
## Алгоритм Хаффмана
Это более эффективный и широко используемый алгоритм, который всегда дает оптимальный код (при использовании целых чисел для длин).
*   **Шаг 1: Создание листьев.** Для каждого символа создается узел дерева с его вероятностью.
*   **Шаг 2: Поиск минимальных.** Находим два узла с наименьшими вероятностями.
*   **Шаг 3: Создание родителя.** Создаем новый внутренний узел, который будет родителем этих двух узлов. Его вероятность равна сумме вероятностей детей.
*   **Шаг 4: Присвоение битов.** Одному ребенку присваивается бит `0`, другому — `1`.
*   **Шаг 5: Повторение.** Удаляем двух детей из списка, добавляем нового родителя. Повторяем шаги 2-4, пока не останется один узел — корень дерева.
*   **Шаг 6: Формирование кодов.** Код для каждого символа — это путь от корня до листа (как в методе двоичного дерева).

**Пример (для тех же символов):**
1.  Исходные узлы: A(0.3), B(0.2), C(0.2), D(0.15), E(0.1), F(0.05)
2.  Берем F(0.05) и E(0.1). Создаем узел FE(0.15). Присваиваем F=`0`, E=`1`.
3.  Теперь узлы: A(0.3), B(0.2), C(0.2), D(0.15), FE(0.15)
4.  Берем D(0.15) и FE(0.15). Создаем узел DFE(0.3). Присваиваем D=`0`, FE=`1`.
5.  Теперь узлы: A(0.3), B(0.2), C(0.2), DFE(0.3)
6.  Берем B(0.2) и C(0.2). Создаем узел BC(0.4). Присваиваем B=`0`, C=`1`.
7.  Теперь узлы: A(0.3), DFE(0.3), BC(0.4)
8.  Берем A(0.3) и DFE(0.3). Создаем узел ADFE(0.6). Присваиваем A=`0`, DFE=`1`.
9.  Теперь узлы: ADFE(0.6), BC(0.4)
10. Берем ADFE(0.6) и BC(0.4). Создаем корень ABCDFE(1.0). Присваиваем ADFE=`0`, BC=`1`.

**Формируем коды, двигаясь от корня к листьям:**
*   A: `00`
*   D: `010`
*   F: `0110`
*   E: `0111`
*   B: `10`
*   C: `11`

**Итоговые коды:**

| Символ | Код  |
|--------|------|
| A      | 00   |
| B      | 10   |
| C      | 11   |
| D      | 010  |
| E      | 0111 |
| F      | 0110 |
Средняя длина кода: `L = 0.3*2 + 0.2*2 + 0.2*2 + 0.15*3 + 0.1*4 + 0.05*4 = 2.45` бит/символ.
**Сравнение:** В данном примере оба алгоритма дали одинаковую среднюю длину, но структура кодов разная. Алгоритм Хаффмана часто дает более сбалансированные коды и является стандартом де-факто.

---
# 5.3 Информационная избыточность и границы для средней длины кодов

## Информационная избыточность
Это мера того, насколько код отличается от идеального (оптимального) кода.
*   **Абсолютная избыточность:** `ΔD = H - L`, где `H` — энтропия источника (теоретический минимум средней длины кода), `L` — фактическая средняя длина кода.
*   **Относительная избыточность:** `δ = ΔD / H = (H - L) / H`.
**Пример:** Для источника с энтропией `H = 2.4` бит/символ и кода со средней длиной `L = 2.45` бит/символ, абсолютная избыточность `ΔD = 0.05` бит/символ, относительная `δ ≈ 2.08%`.
## Границы для средней длины кода
*   **Нижняя граница (теорема Шеннона):** Для любого кода (не обязательно префиксного) средняя длина `L` должна удовлетворять условию `L >= H`, где `H` — энтропия источника. Это означает, что невозможно построить код, средняя длина которого меньше энтропии.
*   **Верхняя граница для префиксных кодов:** Для любого префиксного кода средняя длина `L` удовлетворяет условию `L < H + 1`. Это означает, что даже в худшем случае, префиксный код не будет сильно хуже оптимального.
**Пример:** Если `H = 2.4`, то для любого кода `L >= 2.4`, а для префиксного кода `L < 3.4`. В нашем примере с алгоритмом Хаффмана `L = 2.45`, что находится между этими границами.
