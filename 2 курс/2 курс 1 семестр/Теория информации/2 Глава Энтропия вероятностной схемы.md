# 2.1. Энтропия, как мера степени неопределенности физической системы
*   **Основная идея:** Энтропия — это количественная мера **неопределенности**, связанной с возможными состояниями физической системы. Чем больше неопределенность (чем больше вариантов исхода), тем выше энтропия.
*   **Примеры:**
    *   **Монета:** Система может находиться в одном из двух состояний: "герб" или "цифра". Неопределенность высока, так как оба исхода равновероятны.
    *   **Игальная кость:** Система может находиться в одном из шести состояний (выпадение 1, 2, 3, 4, 5 или 6 очков). Неопределенность еще выше, чем у монеты, так как вариантов больше.
*   **Связь с информацией:** Количество информации, получаемой при наблюдении за системой, равно уменьшению неопределенности. Если система была полностью неопределенной (энтропия максимальна), то после наблюдения мы получаем максимальное количество информации.
*   **Формализация:** Для системы, которая может находиться в `n` состояниях `x_i` с вероятностями `p(x_i)`, энтропия `H(X)` определяется формулой Шеннона:
    `H(X) = - Σ p(x_i) * log₂(p(x_i))`, где суммирование ведется по всем `i` от 1 до `n`.
---
# 2.2. Единицы измерения энтропии
*   **Бит (bit):** Основная единица измерения энтропии и информации в теории информации. Один бит — это количество информации, которое устраняет неопределенность между двумя равновероятными событиями (например, выпадение "герба" или "цифры" при бросании монеты).
*   **Нат (nat):** Альтернативная единица, основанная на натуральном логарифме (`ln`). `1 нат ≈ 1.4427 бита`.
*   **Хартли (hartley):** Единица, основанная на десятичном логарифме (`log₁₀`). `1 хартли ≈ 3.3219 бита`.
*   **Связь между единицами:**
    *   `1 бит = ln(2) нат ≈ 0.693 нат`
    *   `1 бит = log₁₀(2) хартли ≈ 0.301 хартли`
---
# 2.3. Основные свойства энтропии простой физической системы
Энтропия обладает рядом важных свойств:
1.  **Неотрицательность:** `H(X) ≥ 0`. Энтропия всегда неотрицательна. Знак равенства достигается только в том случае, если одно из состояний достоверно (его вероятность равна 1, а все остальные — 0). В этом случае неопределенность отсутствует, и информация, получаемая при наблюдении, равна нулю.
2.  **Максимальное значение при равновероятности:** При заданном числе состояний `n` энтропия `H(X)` максимальна и равна `log₂(n)` тогда и только тогда, когда все состояния равновероятны (`p(x_i) = 1/n` для всех `i`). Это означает, что наибольшая неопределенность (и, следовательно, наибольшее потенциальное количество информации) возникает, когда все исходы одинаково вероятны.
3.  **Аддитивность для независимых систем:** Если две системы `X` и `Y` независимы, то энтропия их объединения равна сумме энтропий каждой системы: `H(X,Y) = H(X) + H(Y)`. Это свойство является одним из ключевых для определения энтропии как меры информации.
4.  **Монотонность:** Энтропия монотонно возрастает с увеличением числа состояний `n` при условии, что они равновероятны.
---
# 2.4. Энтропия и математическое ожидание
*   **Связь:** Энтропия `H(X)` можно интерпретировать как **математическое ожидание** собственной информации `I(x_i)` каждого состояния `x_i`.
    *   **Собственная информация (Information Content):** Количество информации, содержащееся в конкретном событии `x_i`, определяется как `I(x_i) = -log₂(p(x_i))`. Чем меньше вероятность события, тем больше его собственная информация.
    *   **Энтропия как среднее значение:** `H(X) = E[I(X)] = Σ p(x_i) * I(x_i) = Σ p(x_i) * (-log₂(p(x_i)))`. Таким образом, энтропия — это среднее количество информации, которое мы ожидаем получить при наблюдении за системой.
*   **Интерпретация:** Эта связь показывает, что энтропия — это не просто абстрактная мера, а величина, имеющая прямое практическое значение. Она позволяет прогнозировать среднее количество информации, которое будет получено при работе с источником данных.
---
# 2.5. Условная энтропия и энтропия объединения
*   **Условная энтропия `H(Y|X)`:** Мера неопределенности системы `Y` при условии, что состояние системы `X` известно. Формула: `H(Y|X) = - Σ Σ p(x_i, y_j) * log₂(p(y_j|x_i))`, где `p(y_j|x_i)` — условная вероятность состояния `y_j` при известном состоянии `x_i`.
    *   **Частная условная энтропия:** `H(y_j|x_i) = -p(y_j|x_i) * log₂(p(y_j|x_i))` — неопределенность `Y` при фиксированном `x_i`.
    *   **Общая условная энтропия:** `H(Y|X) = Σ p(x_i) * H(Y|x_i)` — средняя неопределенность `Y` по всем возможным состояниям `X`.
*   **Энтропия объединения `H(X,Y)`:** Мера неопределенности двух систем `X` и `Y`, рассматриваемых вместе. Формула: `H(X,Y) = - Σ Σ p(x_i, y_j) * log₂(p(x_i, y_j))`.
*   **Связь между энтропиями:**
    *   `H(X,Y) = H(X) + H(Y|X)`
    *   `H(X,Y) = H(Y) + H(X|Y)`
    *   Отсюда следует, что `H(Y|X) = H(X,Y) - H(X)` и `H(X|Y) = H(X,Y) - H(Y)`.
*   **Взаимная информация `I(X;Y)`:** Количество информации, которое одна система содержит о другой. Вычисляется как `I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)`. Это мера зависимости между системами. Если системы независимы, то `I(X;Y) = 0`.