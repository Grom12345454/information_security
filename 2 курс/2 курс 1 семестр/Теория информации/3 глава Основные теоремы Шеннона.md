# 3.1. Количество информации в дискретном сообщении. Дискретные источники сообщений без памяти и с памятью
*   **Источник сообщений (ИС):** Устройство или процесс, генерирующее последовательность символов (сообщений) из некоторого алфавита.
*   **Дискретный источник:** Источник, который выдает сообщения, состоящие из отдельных, не связанных между собой элементарных символов.
*   **Источник без памяти:** Вероятность появления текущего символа не зависит от ранее выданных символов. Символы независимы.
    *   Пример: Бросание идеальной монеты или кубика.
*   **Источник с памятью:** Вероятность появления текущего символа зависит от одного или нескольких предыдущих символов. Символы зависимы.
    *   Пример: Текст на естественном языке, где вероятность следующей буквы зависит от предыдущих (например, после "П" часто идет "Р").
*   **Энтропия источника (H(U)):** Мера среднего количества информации, приходящегося на один символ источника. Для источника без памяти она равна энтропии алфавита: `H(U) = - Σ p_i * log₂(p_i)`, где `p_i` — вероятность появления i-го символа.
---
# 3.2. Первая теорема Шеннона (Теорема о кодировании для канала без шума)
Эта теорема устанавливает пределы сжатия информации без потерь.
## Прямая теорема Шеннона
*   **Формулировка:** Для любого дискретного источника информации без памяти со средней энтропией `H(U)` и для любого сколь угодно малого положительного числа `ξ > 0` существует способ кодирования, при котором среднее число двоичных знаков (битов), приходящихся на один символ исходного сообщения, будет меньше `H(U) + ξ`.
*   **Интерпретация:** Можно сжать сообщение так, чтобы средняя длина кодового слова была сколь угодно близка к энтропии источника, но никогда не меньше ее. Энтропия `H(U)` — это абсолютный нижний предел сжатия.
*   **Методы доказательства:**
    1.  **Метод Фано (Оптимальное кодирование):** Строит префиксный код, при котором более вероятные символы получают более короткие кодовые слова, а менее вероятные — более длинные. Это позволяет минимизировать среднюю длину кода.
        *   **Алгоритм Фано:**
            1.  Символы алфавита располагаются в порядке убывания их вероятностей.
            2.  Множество символов делится на две группы так, чтобы суммарные вероятности групп были как можно ближе друг к другу.
            3.  Первой группе присваивается бит `0`, второй — `1`.
            4.  Каждая группа снова делится на подгруппы (по тому же принципу), и к каждому кодовому слову добавляется новый бит (`0` для первой подгруппы, `1` для второй). Процесс повторяется до тех пор, пока в каждой подгруппе не останется по одному символу.
        *   **Пример:** Для алфавита `{A, B, C, D}` с вероятностями `{0.5, 0.25, 0.125, 0.125}` метод Фано даст коды: `A=0`, `B=10`, `C=110`, `D=111`. Средняя длина `L = 0.5*1 + 0.25*2 + 0.125*3 + 0.125*3 = 1.75` бит/символ, что близко к энтропии `H = 1.75` бит/символ.
    2.  **Метод Шеннона (Дробная часть):** Строит код, основанный на представлении кумулятивной вероятности символа в виде двоичной дроби. Кодом символа является начальный отрезок этой дроби, длина которого определяется формулой `l_i = ⌈-log₂(p_i)⌉`.
## Обратная теорема Шеннона
*   **Формулировка:** Невозможно построить код, при котором среднее число битов на символ было бы меньше энтропии источника `H(U)`.
*   **Интерпретация:** Энтропия `H(U)` — это фундаментальный предел, который нельзя превзойти. Любая попытка сжать данные ниже этого предела неизбежно приведет к потере информации (необратимому сжатию).
*   **Доказательство:** Основывается на том, что количество возможных кодовых комбинаций длины `L` равно `2^L`. Если `L < H(U)`, то количество кодовых комбинаций недостаточно для уникального кодирования всех возможных сообщений источника, что приводит к неоднозначности декодирования.
## Практическое применение первой теоремы
*   **Основа для сжатия данных:** Все алгоритмы сжатия без потерь (ZIP, RAR, PNG, MP3, JPEG) основаны на идее первой теоремы Шеннона. Они используют статистические свойства данных (частоту появления символов, слов, пикселей) для построения эффективных кодов.
*   **Оптимальное кодирование:** Алгоритмы Хаффмана и Шеннона-Фано являются практической реализацией первой теоремы.
---
# 3.3. Вторая теорема Шеннона (Теорема о кодировании для канала с шумом)
Эта теорема устанавливает пределы надежной передачи информации по каналам связи, подверженным помехам.
## Прямая теорема Шеннона
*   **Формулировка:** Для любого дискретного постоянного канала связи с пропускной способностью `C` и для любого сколь угодно малого положительного числа `ε > 0` существует способ кодирования, при котором скорость передачи информации `V` может быть сделана сколь угодно близкой к `C` (`V > C - ε`) и при этом вероятность ошибки при декодировании стремится к нулю (`P_e → 0`).
*   **Интерпретация:** Можно передавать информацию по зашумленному каналу со скоростью, сколь угодно близкой к пропускной способности канала, с практически нулевой вероятностью ошибки, если использовать достаточно сложные коды (обычно длинные блоковые коды).
*   **Ключевые понятия:**
    *   **Пропускная способность канала (C):** Максимальное количество информации, которое можно передать по каналу в единицу времени без потерь. Для дискретного канала без памяти `C = max I(A;B)`, где `I(A;B)` — взаимная информация между входом `A` и выходом `B` канала.
    *   **Взаимная информация (I(A;B)):** Мера количества информации, передаваемой по каналу. `I(A;B) = H(A) - H(A|B) = H(B) - H(B|A) = H(A) + H(B) - H(AB)`.
    *   **Условная энтропия (H(A|B)):** Неопределенность входного сигнала `A`, когда известен выходной сигнал `B`. Она характеризует потери информации в канале из-за помех.
*   **Доказательство (основная идея):** Используется метод случайного кодирования. Генерируется большое множество случайных кодовых слов. Показывается, что для большинства из них вероятность ошибки при декодировании мала, если скорость передачи `V` меньше пропускной способности `C`.
## Обратная теорема Шеннона
*   **Формулировка:** Невозможно передавать информацию по каналу со скоростью `V > C` с вероятностью ошибки, стремящейся к нулю.
*   **Интерпретация:** Пропускная способность `C` — это абсолютный предел скорости передачи информации по данному каналу. Любая попытка превысить этот предел неизбежно приведет к росту вероятности ошибок.
*   **Доказательство:** Основывается на том, что при `V > C` количество возможных сообщений, которые нужно передать, превышает количество различимых сигналов, которые можно получить на выходе канала из-за помех. Это приводит к неизбежным ошибкам декодирования.
## Следствие из второй теоремы Шеннона
*   **Критерий надежности:** Для того чтобы обеспечить надежную передачу информации по каналу с шумом, необходимо, чтобы производительность источника (скорость генерации информации) не превышала пропускную способность канала. Если `H(U) > C`, то надежная передача невозможна.
*   **Значение:** Теорема дает теоретическую основу для проектирования систем связи. Она показывает, что даже в условиях сильных помех можно достичь надежной передачи, используя соответствующие коды и подходящую скорость передачи.

---
# 3.4. Статистический анализ случайных последовательностей. Энтропийные и информационные характеристики случайных последовательностей
*   **Последовательность:** Упорядоченный набор символов, сгенерированный источником.
*   **Типичная последовательность:** Последовательность, в которой частота появления каждого символа близка к его вероятности появления в источнике. При большой длине последовательности почти все сгенерированные последовательности будут типичными.
*   **Множество типичных последовательностей (T_n(δ)):** Множество всех последовательностей длины `n`, для которых средняя собственная информация (логарифм обратной вероятности) близка к энтропии источника `H(X)`.
*   **Свойства типичных последовательностей:**
    1.  **Вероятность:** Асимптотически почти все последовательности типичны, т.е. `P(T_n(δ)) → 1` при `n → ∞`.
    2.  **Количество:** Количество типичных последовательностей примерно равно `2^{nH(X)}`.
    3.  **Вероятность элемента:** Все типичные последовательности имеют примерно одинаковую вероятность, равную `2^{-nH(X)}`.
*   **Применение:** Понятие типичных последовательностей используется для упрощения анализа и доказательства теорем Шеннона. Оно позволяет сосредоточиться только на наиболее вероятных последовательностях, игнорируя маловероятные.
